{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRe6S30b9Ng7"
   },
   "source": [
    "# TD 4: Mitigation des biais avec des méthodes de pré-processing et de post-processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation of the environnement\n",
    "\n",
    "We highly recommend you to follow these steps, it will allow every student to work in an environment as similar as possible to the one used during testing.\n",
    "\n",
    "### Colab Settings\n",
    "  The next cell of code are to execute only once per colab environment\n",
    "\n",
    "\n",
    "#### Python env creation\n",
    "\n",
    "        ```\n",
    "        ! python -m pip install numpy fairlearn plotly nbformat ipykernel aif360[\"inFairness\"] aif360['AdversarialDebiasing'] causal-learn BlackBoxAuditing cvxpy dice-ml lime shapkit\n",
    "        ```\n",
    "### Local Settings\n",
    "\n",
    "#### 1. Uv installation\n",
    "\n",
    "\n",
    "        https://docs.astral.sh/uv/getting-started/installation/\n",
    "\n",
    "\n",
    "        `curl -LsSf https://astral.sh/uv/install.sh | sh`\n",
    "\n",
    "        Python version 3.12 installation (highly recommended)\n",
    "        `uv python install 3.12`\n",
    "\n",
    "\n",
    "#### 3. Python env creation\n",
    "\n",
    "        ```\n",
    "        mkdir TD_bias_mitigation\n",
    "        cd TD_bias_mitigation\n",
    "        uv python pin 3.12\n",
    "        uv init\n",
    "        uv pip install numpy fairlearn plotly nbformat ipykernel aif360[\"inFairness\"] aif360['AdversarialDebiasing'] causal-learn BlackBoxAuditing cvxpy dice-ml lime shapkit\n",
    "        ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To execute only in Colab\n",
    "# ! python -m pip install numpy fairlearn plotly nbformat ipykernel aif360[\"inFairness\"] aif360['AdversarialDebiasing'] causal-learn BlackBoxAuditing cvxpy dice-ml lime shapkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to compute fairness metrics using aif360\n",
    "\n",
    "from aif360.sklearn.metrics import *\n",
    "from sklearn.metrics import  balanced_accuracy_score\n",
    "\n",
    "\n",
    "# This method takes lists\n",
    "def get_metrics(\n",
    "    y_true, # list or np.array of truth values\n",
    "    y_pred=None,  # list or np.array of predictions\n",
    "    prot_attr=None, # list or np.array of protected/sensitive attribute values\n",
    "    priv_group=1, # value taken by the privileged group\n",
    "    pos_label=1, # value taken by the positive truth/prediction\n",
    "    sample_weight=None # list or np.array of weights value,\n",
    "):\n",
    "    group_metrics = {}\n",
    "    group_metrics[\"base_rate_truth\"] = base_rate(\n",
    "        y_true=y_true, pos_label=pos_label, sample_weight=sample_weight\n",
    "    )\n",
    "    group_metrics[\"statistical_parity_difference\"] = statistical_parity_difference(\n",
    "        y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, priv_group=priv_group, pos_label=pos_label, sample_weight=sample_weight\n",
    "    )\n",
    "    group_metrics[\"disparate_impact_ratio\"] = disparate_impact_ratio(\n",
    "        y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, priv_group=priv_group, pos_label=pos_label, sample_weight=sample_weight\n",
    "    )\n",
    "    if not y_pred is None:\n",
    "        group_metrics[\"base_rate_preds\"] = base_rate(\n",
    "        y_true=y_pred, pos_label=pos_label, sample_weight=sample_weight\n",
    "        )\n",
    "        group_metrics[\"equal_opportunity_difference\"] = equal_opportunity_difference(\n",
    "            y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, priv_group=priv_group, pos_label=pos_label, sample_weight=sample_weight\n",
    "        )\n",
    "        group_metrics[\"average_odds_difference\"] = average_odds_difference(\n",
    "            y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, priv_group=priv_group, pos_label=pos_label, sample_weight=sample_weight\n",
    "        )\n",
    "        if len(set(y_pred))>1:\n",
    "            group_metrics[\"conditional_demographic_disparity\"] = conditional_demographic_disparity(\n",
    "                y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, pos_label=pos_label, sample_weight=sample_weight\n",
    "            )\n",
    "        else:\n",
    "            group_metrics[\"conditional_demographic_disparity\"] =None\n",
    "        group_metrics[\"smoothed_edf\"] = smoothed_edf(\n",
    "        y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, pos_label=pos_label, sample_weight=sample_weight\n",
    "        )\n",
    "        group_metrics[\"df_bias_amplification\"] = df_bias_amplification(\n",
    "        y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, pos_label=pos_label, sample_weight=sample_weight\n",
    "        )\n",
    "        group_metrics[\"balanced_accuracy_score\"] = balanced_accuracy_score(\n",
    "        y_true=y_true, y_pred=y_pred, sample_weight=sample_weight\n",
    "        )\n",
    "    return group_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9c9ITNS89NhA"
   },
   "source": [
    "## 1.Manipulate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1707202697233,
     "user": {
      "displayName": "Alice H",
      "userId": "13901604971984976961"
     },
     "user_tz": -60
    },
    "id": "88mMZIic9NhB",
    "outputId": "f0be967d-0531-4dd0-8d13-39accc70509e"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", append=True, category=UserWarning)\n",
    "# Datasets\n",
    "from aif360.datasets import MEPSDataset19\n",
    "from aif360.explainers import MetricTextExplainer\n",
    "\n",
    "# Fairness metrics\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "\n",
    "\n",
    "MEPSDataset19_data = MEPSDataset19()\n",
    "(dataset_orig_panel19_train, dataset_orig_panel19_val, dataset_orig_panel19_test) = (\n",
    "    MEPSDataset19().split([0.5, 0.8], shuffle=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 20083,
     "status": "ok",
     "timestamp": 1707202719711,
     "user": {
      "displayName": "Alice H",
      "userId": "13901604971984976961"
     },
     "user_tz": -60
    },
    "id": "LYbdfIPs9NhE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7915, 4749, 3166)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_orig_panel19_train.instance_weights), len(\n",
    "    dataset_orig_panel19_val.instance_weights\n",
    "), len(dataset_orig_panel19_test.instance_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21854.981705, 18169.604822, 17191.832515, ...,  3896.116219,\n",
       "        4883.851005,  6630.588948], shape=(15830,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance_weights = MEPSDataset19_data.instance_weights\n",
    "instance_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Taille du dataset 15830, poids total du dataset 141367240.546316.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Taille du dataset {len(instance_weights)}, poids total du dataset {instance_weights.sum()}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion en dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(MepsDataset):\n",
    "    data = MepsDataset.convert_to_dataframe()\n",
    "    # data_train est un tuple, avec le data_frame et un dictionnaire avec toutes les infos (poids, attributs sensibles etc)\n",
    "    df = data[0]\n",
    "    df[\"WEIGHT\"] = data[1][\"instance_weights\"]\n",
    "    return df\n",
    "\n",
    "\n",
    "df = get_df(MEPSDataset19_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous réalisons maintenant l'opération inverse (qui sera indispensable pour le projet). Créer un objet de la classe StandardDataset de AIF360 à partir du dataframe. \n",
    "\n",
    "Pour le projet cela vous permettre d'utiliser les méthode déjà implémentées dans AIF360 sur votre jeu de données.\n",
    "\n",
    "Ici cela n'a aucun intéret car le dataframe vien d'un StandardDataset, nous vous fournissons le code. Mais cela vaut le coup de le lire attentivement et de poser des questions si besoin.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from aif360.datasets import StandardDataset\n",
    "import pandas as pd\n",
    "\n",
    "# Get categorical column from one hot encoding (specitic to MEPSdataset)\n",
    "# Here we create a dictionnary that links each categorical column name\n",
    "# to the list of corresponding one hot encoded columns\n",
    "categorical_columns_dic = {}\n",
    "for col in df.columns:\n",
    "    col_split = col.split(\"=\")\n",
    "    if len(col_split) > 1:\n",
    "        cat_col = col_split[0]\n",
    "        if not (cat_col in categorical_columns_dic.keys()):\n",
    "            categorical_columns_dic[cat_col] = []\n",
    "        categorical_columns_dic[cat_col].append(col)\n",
    "categorical_features = categorical_columns_dic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15830, 140)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15830, 43)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we recreate the categorical column value from the one hot encoded\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "def categorical_transform(df, onehotencoded, cat_col):\n",
    "    if len(onehotencoded) > 1:\n",
    "        return df[onehotencoded].apply(\n",
    "            lambda x: onehotencoded[np.argmax(x)][len(cat_col) + 1 :], axis=1\n",
    "        )\n",
    "    else:\n",
    "        return df[onehotencoded]\n",
    "\n",
    "\n",
    "# Reverse the categorical one hot encoded\n",
    "for cat_col, onehotencoded in categorical_columns_dic.items():\n",
    "    df[cat_col] = categorical_transform(df, onehotencoded, cat_col)\n",
    "    df.drop(columns=onehotencoded, inplace=True)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyDataset = StandardDataset(\n",
    "    df=df,\n",
    "    label_name=\"UTILIZATION\",\n",
    "    favorable_classes=[1],\n",
    "    protected_attribute_names=[\"RACE\"],\n",
    "    privileged_classes=[[1]],\n",
    "    instance_weights_name=\"WEIGHT\",\n",
    "    categorical_features=categorical_features,\n",
    "    features_to_keep=[],\n",
    "    features_to_drop=[],\n",
    "    na_values=[\"?\", \"Unknown/Invalid\"],\n",
    "    custom_preprocessing=None,\n",
    "    metadata=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49826823461176517 0.21507139363038463\n",
      "0.49826823461176517 0.21507139363038463\n"
     ]
    }
   ],
   "source": [
    "# We check the dataset has the same metrics :D\n",
    "# Attention étonnanement le positive label 'favorable_classes' est par défaut 1 (cela est un peu bizarre pour ce dataset)\n",
    "print(\n",
    "    BinaryLabelDatasetMetric(\n",
    "        MEPSDataset19_data,\n",
    "        unprivileged_groups=[{\"RACE\": 0}],\n",
    "        privileged_groups=[{\"RACE\": 1}],\n",
    "    ).disparate_impact(),\n",
    "    BinaryLabelDatasetMetric(\n",
    "        MEPSDataset19_data,\n",
    "        unprivileged_groups=[{\"RACE\": 0}],\n",
    "        privileged_groups=[{\"RACE\": 1}],\n",
    "    ).base_rate(),\n",
    ")\n",
    "print(\n",
    "    BinaryLabelDatasetMetric(\n",
    "        MyDataset, unprivileged_groups=[{\"RACE\": 0}], privileged_groups=[{\"RACE\": 1}]\n",
    "    ).disparate_impact(),\n",
    "    BinaryLabelDatasetMetric(\n",
    "        MyDataset, unprivileged_groups=[{\"RACE\": 0}], privileged_groups=[{\"RACE\": 1}]\n",
    "    ).base_rate(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4982682346117653, np.float64(0.21507139363038463))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aif360.sklearn.metrics import disparate_impact_ratio, base_rate\n",
    "\n",
    "dir = disparate_impact_ratio(\n",
    "    y_true=df.UTILIZATION, prot_attr=df.RACE, pos_label=1, sample_weight=df.WEIGHT\n",
    ")\n",
    "br = base_rate(y_true=df.UTILIZATION, pos_label=1, sample_weight=df.WEIGHT)\n",
    "dir, br"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3Zbw-U4mTKf"
   },
   "source": [
    "## 2. Appliquer les méthodes de pré-processing disponibles dans AIF360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('RACE', [{'RACE': np.float64(0.0)}], [{'RACE': np.float64(1.0)}])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sens_ind = 0\n",
    "sens_attr = dataset_orig_panel19_train.protected_attribute_names[sens_ind]\n",
    "unprivileged_groups = [\n",
    "    {sens_attr: v}\n",
    "    for v in dataset_orig_panel19_train.unprivileged_protected_attributes[sens_ind]\n",
    "]\n",
    "privileged_groups = [\n",
    "    {sens_attr: v}\n",
    "    for v in dataset_orig_panel19_train.privileged_protected_attributes[sens_ind]\n",
    "]\n",
    "sens_attr, unprivileged_groups, privileged_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Quesiton: Apprendre une regression logistique qui prédit l'UTILIZATION\n",
    "\n",
    "Attention nous avons enlever le preprocessing sur le dataframe, il faut cette fois utiliser l'API d'AIF360\n",
    "https://aif360.readthedocs.io/en/latest/modules/generated/aif360.datasets.StructuredDataset.html\n",
    "\n",
    "pour retrouver les features (X), les labels (y) et les poids de chaque instance du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8395130386823652"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "X_train = dataset_orig_panel19_train.features\n",
    "y_train = dataset_orig_panel19_train.labels[:,0]\n",
    "X_val = dataset_orig_panel19_val.features\n",
    "y_val = dataset_orig_panel19_val.labels[:,0]\n",
    "\n",
    "\n",
    "model = make_pipeline(StandardScaler(), LogisticRegression(solver='liblinear', random_state=42))\n",
    "\n",
    "model = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    **{\"logisticregression__sample_weight\": dataset_orig_panel19_train.instance_weights}\n",
    ")\n",
    "\n",
    "preds = model.predict(X_val)\n",
    "\n",
    "model.score(X_val, y_val, sample_weight=dataset_orig_panel19_val.instance_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8395130386823652, 0.708840349124986)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_val, preds, sample_weight=dataset_orig_panel19_val.instance_weights), balanced_accuracy_score(y_val, preds, sample_weight=dataset_orig_panel19_val.instance_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Question: Calcul des métriques de fairness\n",
    "\n",
    "Calculer les métriques du dataset de validation seul.\n",
    "\n",
    "Calculer les métriques basées sur les prédictions et la vérité du dataset de validation.\n",
    "\n",
    "En comparaison calculer les métriques basées sur des prédictions aléatoires et la vérité du dataset de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_rate_truth': np.float64(0.2138698873472541),\n",
       " 'statistical_parity_difference': np.float64(-0.13584669171021266),\n",
       " 'disparate_impact_ratio': 0.4910855710688648}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metrics on validation dataset\n",
    "get_metrics(\n",
    "        y_true=y_val,\n",
    "        y_pred=None,\n",
    "        prot_attr=dataset_orig_panel19_val.protected_attributes[:, sens_ind],\n",
    "        pos_label=1,\n",
    "        sample_weight=dataset_orig_panel19_val.instance_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_rate_truth': np.float64(0.2138698873472541),\n",
       " 'statistical_parity_difference': np.float64(-0.14093443635350933),\n",
       " 'disparate_impact_ratio': 0.3198008242753057,\n",
       " 'base_rate_preds': np.float64(0.15214409985729335),\n",
       " 'equal_opportunity_difference': -0.20724951359870203,\n",
       " 'average_odds_difference': -0.13465510387778157,\n",
       " 'conditional_demographic_disparity': np.float64(-0.056892160491250884),\n",
       " 'smoothed_edf': np.float64(1.140056560441825),\n",
       " 'df_bias_amplification': np.float64(0.42891980941410734),\n",
       " 'balanced_accuracy_score': 0.708840349124986}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metrics based on predictions and truth on validation dataset\n",
    "get_metrics(\n",
    "        y_true=y_val,\n",
    "        y_pred=preds,\n",
    "        prot_attr=dataset_orig_panel19_val.protected_attributes[:, sens_ind],\n",
    "        pos_label=1,\n",
    "        sample_weight=dataset_orig_panel19_val.instance_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_rate_truth': np.float64(0.2138698873472541),\n",
       " 'statistical_parity_difference': np.float64(0.045059590006641004),\n",
       " 'disparate_impact_ratio': 1.095834904133213,\n",
       " 'base_rate_preds': np.float64(0.48778047281512177),\n",
       " 'equal_opportunity_difference': 0.06340701258640247,\n",
       " 'average_odds_difference': 0.05064364360085588,\n",
       " 'conditional_demographic_disparity': np.float64(0.00939115827286097),\n",
       " 'smoothed_edf': np.float64(0.09151653802208826),\n",
       " 'df_bias_amplification': np.float64(-0.6196202130056293),\n",
       " 'balanced_accuracy_score': 0.48530498218618456}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metrics based on random predictions and truth on validation dataset\n",
    "\n",
    "pred_random = np.array([int(r*2) for r in np.random.random(size=len(y_val)).tolist()])\n",
    "pred_random.max(), pred_random.min()\n",
    "get_metrics(\n",
    "        y_true=y_val,\n",
    "        y_pred=pred_random,\n",
    "        prot_attr=dataset_orig_panel19_val.protected_attributes[:, sens_ind],\n",
    "        pos_label=1,\n",
    "        sample_weight=dataset_orig_panel19_val.instance_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Repondération\n",
    "#### 2.2.1. Question : Trouver dans l'API quels objets/fonctions sont à utiliser pour faire de repondération et les appliquer sur le dataset d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.preprocessing import *\n",
    "\n",
    "RW = Reweighing(\n",
    "    unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "RW.fit(dataset_orig_panel19_train)\n",
    "dataset_transf_train = RW.transform(dataset_orig_panel19_train)\n",
    "dataset_transf_val = RW.transform(dataset_orig_panel19_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Question: Apprendre une regression logistique sur les données pondérées et calculer les métriques de fairness sur l'échantillon de validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme vu en cours le Reweighting ne modifie que la pondération du dataset, les features et label restent inchangés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8612339439882081, 0.8330705544304328)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rw = make_pipeline(StandardScaler(), LogisticRegression(solver='liblinear', random_state=42))\n",
    "\n",
    "model_rw = model_rw.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    **{\"logisticregression__sample_weight\": dataset_transf_train.instance_weights}\n",
    ")\n",
    "\n",
    "preds_rw = model_rw.predict(X_val)\n",
    "\n",
    "model_rw.score(X_val, y_val), model_rw.score(X_val, y_val, sample_weight=dataset_transf_val.instance_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8326581068729562, 0.6886110023890155)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_val, preds_rw, sample_weight=dataset_orig_panel19_val.instance_weights), balanced_accuracy_score(y_val, preds_rw, sample_weight=dataset_orig_panel19_val.instance_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_rate_truth': np.float64(0.21226910887324338),\n",
       " 'statistical_parity_difference': np.float64(-0.014255553664911602),\n",
       " 'disparate_impact_ratio': 0.9024495192759333,\n",
       " 'base_rate_preds': np.float64(0.14057984998837425),\n",
       " 'equal_opportunity_difference': 0.007035005711166775,\n",
       " 'average_odds_difference': -0.00590625240601339,\n",
       " 'conditional_demographic_disparity': np.float64(-0.006190961230538837),\n",
       " 'smoothed_edf': np.float64(0.10264244923839327),\n",
       " 'df_bias_amplification': np.float64(0.09153838089694943),\n",
       " 'balanced_accuracy_score': 0.6887406389747808}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metrics on prediction with RW on validation dataset\n",
    "get_metrics(\n",
    "    y_true=y_val,\n",
    "    y_pred=preds_rw,\n",
    "    prot_attr=dataset_orig_panel19_val.protected_attributes[:, sens_ind],\n",
    "    pos_label=1,\n",
    "    sample_weight=dataset_transf_val.instance_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Disparate Impact Remover\n",
    "#### 2.3.1. Question : Trouver dans l'API quels objets/fonctions sont à utiliser pour faire une approache de disparate impact remover et les appliquer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = DisparateImpactRemover(repair_level=1., sensitive_attribute='RACE')\n",
    "train_dir = DIR.fit_transform(dataset_orig_panel19_train)\n",
    "val_dir = DIR.fit_transform(dataset_orig_panel19_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               instance weights features                                    \\\n",
       "                                         protected attribute                 \n",
       "                                     AGE                RACE  PCS42  MCS42   \n",
       "instance names                                                               \n",
       "2301                7146.545007     55.0                 0.0  57.49  54.20   \n",
       "9495                4107.337349     38.0                 0.0  54.22  55.39   \n",
       "9027               15116.392383     63.0                 1.0  51.26  35.33   \n",
       "9168                3341.885401     18.0                 0.0  42.40  63.27   \n",
       "8753                3556.883879     18.0                 0.0  56.71  62.39   \n",
       "...                         ...      ...                 ...    ...    ...   \n",
       "41                 19250.951023     54.0                 1.0  56.15  57.16   \n",
       "13297               2626.412576     39.0                 0.0  50.30  55.61   \n",
       "4006                5081.599952     85.0                 0.0  20.25  36.27   \n",
       "5265                2096.101565      9.0                 0.0  -1.00  -1.00   \n",
       "12499               3529.378673     17.0                 0.0  -1.00  -1.00   \n",
       "\n",
       "                                                            ...          \\\n",
       "                                                            ...           \n",
       "               K6SUM42 REGION=1 REGION=2 REGION=3 REGION=4  ... EMPST=4   \n",
       "instance names                                              ...           \n",
       "2301               1.0      1.0      0.0      0.0      0.0  ...     0.0   \n",
       "9495               0.0      0.0      1.0      0.0      0.0  ...     0.0   \n",
       "9027              12.0      0.0      0.0      1.0      0.0  ...     1.0   \n",
       "9168               0.0      0.0      1.0      0.0      0.0  ...     0.0   \n",
       "8753               0.0      0.0      0.0      0.0      1.0  ...     1.0   \n",
       "...                ...      ...      ...      ...      ...  ...     ...   \n",
       "41                 0.0      0.0      0.0      1.0      0.0  ...     0.0   \n",
       "13297              0.0      0.0      0.0      0.0      1.0  ...     0.0   \n",
       "4006              14.0      1.0      0.0      0.0      0.0  ...     1.0   \n",
       "5265              -1.0      0.0      0.0      0.0      1.0  ...     0.0   \n",
       "12499             -1.0      0.0      0.0      1.0      0.0  ...     1.0   \n",
       "\n",
       "                                                                               \\\n",
       "                                                                                \n",
       "               POVCAT=1 POVCAT=2 POVCAT=3 POVCAT=4 POVCAT=5 INSCOV=1 INSCOV=2   \n",
       "instance names                                                                  \n",
       "2301                0.0      0.0      0.0      0.0      1.0      1.0      0.0   \n",
       "9495                1.0      0.0      0.0      0.0      0.0      0.0      1.0   \n",
       "9027                0.0      0.0      0.0      1.0      0.0      0.0      1.0   \n",
       "9168                0.0      0.0      1.0      0.0      0.0      0.0      1.0   \n",
       "8753                0.0      1.0      0.0      0.0      0.0      0.0      0.0   \n",
       "...                 ...      ...      ...      ...      ...      ...      ...   \n",
       "41                  0.0      0.0      0.0      0.0      1.0      1.0      0.0   \n",
       "13297               0.0      0.0      0.0      1.0      0.0      1.0      0.0   \n",
       "4006                0.0      1.0      0.0      0.0      0.0      0.0      1.0   \n",
       "5265                1.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "12499               1.0      0.0      0.0      0.0      0.0      0.0      1.0   \n",
       "\n",
       "                        labels  \n",
       "                                \n",
       "               INSCOV=3         \n",
       "instance names                  \n",
       "2301                0.0    1.0  \n",
       "9495                0.0    0.0  \n",
       "9027                0.0    0.0  \n",
       "9168                0.0    0.0  \n",
       "8753                1.0    0.0  \n",
       "...                 ...    ...  \n",
       "41                  0.0    0.0  \n",
       "13297               0.0    0.0  \n",
       "4006                0.0    1.0  \n",
       "5265                1.0    0.0  \n",
       "12499               0.0    0.0  \n",
       "\n",
       "[7915 rows x 140 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_orig_panel19_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               instance weights features                                    \\\n",
       "                                         protected attribute                 \n",
       "                                     AGE                RACE  PCS42  MCS42   \n",
       "instance names                                                               \n",
       "2301                7146.545007     55.0                 0.0  57.49  54.19   \n",
       "9495                4107.337349     38.0                 0.0  54.22  55.39   \n",
       "9027               15116.392383     63.0                 1.0  50.73  34.87   \n",
       "9168                3341.885401     18.0                 0.0  41.42  63.27   \n",
       "8753                3556.883879     18.0                 0.0  56.71  62.36   \n",
       "...                         ...      ...                 ...    ...    ...   \n",
       "41                 19250.951023     54.0                 1.0  55.35  56.70   \n",
       "13297               2626.412576     39.0                 0.0  50.28  55.61   \n",
       "4006                5081.599952     85.0                 0.0  19.09  36.27   \n",
       "5265                2096.101565      9.0                 0.0  -1.00  -1.00   \n",
       "12499               3529.378673     17.0                 0.0  -1.00  -1.00   \n",
       "\n",
       "                                                            ...          \\\n",
       "                                                            ...           \n",
       "               K6SUM42 REGION=1 REGION=2 REGION=3 REGION=4  ... EMPST=4   \n",
       "instance names                                              ...           \n",
       "2301               1.0      1.0      0.0      0.0      0.0  ...     0.0   \n",
       "9495               0.0      0.0      1.0      0.0      0.0  ...     0.0   \n",
       "9027              12.0      0.0      0.0      1.0      0.0  ...     1.0   \n",
       "9168               0.0      0.0      1.0      0.0      0.0  ...     0.0   \n",
       "8753               0.0      0.0      0.0      0.0      1.0  ...     1.0   \n",
       "...                ...      ...      ...      ...      ...  ...     ...   \n",
       "41                 0.0      0.0      0.0      1.0      0.0  ...     0.0   \n",
       "13297              0.0      0.0      0.0      0.0      1.0  ...     0.0   \n",
       "4006              14.0      1.0      0.0      0.0      0.0  ...     1.0   \n",
       "5265              -1.0      0.0      0.0      0.0      1.0  ...     0.0   \n",
       "12499             -1.0      0.0      0.0      1.0      0.0  ...     1.0   \n",
       "\n",
       "                                                                               \\\n",
       "                                                                                \n",
       "               POVCAT=1 POVCAT=2 POVCAT=3 POVCAT=4 POVCAT=5 INSCOV=1 INSCOV=2   \n",
       "instance names                                                                  \n",
       "2301                0.0      0.0      0.0      0.0      1.0      1.0      0.0   \n",
       "9495                1.0      0.0      0.0      0.0      0.0      0.0      1.0   \n",
       "9027                0.0      0.0      0.0      1.0      0.0      0.0      1.0   \n",
       "9168                0.0      0.0      1.0      0.0      0.0      0.0      1.0   \n",
       "8753                0.0      1.0      0.0      0.0      0.0      0.0      0.0   \n",
       "...                 ...      ...      ...      ...      ...      ...      ...   \n",
       "41                  0.0      0.0      0.0      0.0      1.0      1.0      0.0   \n",
       "13297               0.0      0.0      0.0      1.0      0.0      1.0      0.0   \n",
       "4006                0.0      1.0      0.0      0.0      0.0      0.0      1.0   \n",
       "5265                1.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "12499               1.0      0.0      0.0      0.0      0.0      0.0      1.0   \n",
       "\n",
       "                        labels  \n",
       "                                \n",
       "               INSCOV=3         \n",
       "instance names                  \n",
       "2301                0.0    1.0  \n",
       "9495                0.0    0.0  \n",
       "9027                0.0    0.0  \n",
       "9168                0.0    0.0  \n",
       "8753                1.0    0.0  \n",
       "...                 ...    ...  \n",
       "41                  0.0    0.0  \n",
       "13297               0.0    0.0  \n",
       "4006                0.0    1.0  \n",
       "5265                1.0    0.0  \n",
       "12499               0.0    0.0  \n",
       "\n",
       "[7915 rows x 140 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2. Question: Apprendre une regression logistique sur les données transformées en retirant l'attribut sensible et calculer les métriques de fairness sur l'échantillon de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8400100853792806"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protected=\"RACE\"\n",
    "index = dataset_orig_panel19_train.feature_names.index(protected)\n",
    "model_dir = make_pipeline(StandardScaler(), LogisticRegression(solver='liblinear', random_state=42))\n",
    "\n",
    "model_dir = model_dir.fit(\n",
    "    np.delete(train_dir.features, index, axis=1),\n",
    "    train_dir.labels[:,0],\n",
    "    **{\"logisticregression__sample_weight\": train_dir.instance_weights}\n",
    ")\n",
    "\n",
    "preds_dir = model_dir.predict(np.delete(val_dir.features, index, axis=1))\n",
    "\n",
    "model_dir.score(np.delete(val_dir.features, index, axis=1), val_dir.labels[:,0], sample_weight=val_dir.instance_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8400100853792806, 0.7053203897748427)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_val, preds_dir, sample_weight=dataset_orig_panel19_val.instance_weights), balanced_accuracy_score(y_val, preds_dir, sample_weight=dataset_orig_panel19_val.instance_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_rate_truth': np.float64(0.2138698873472541),\n",
       " 'statistical_parity_difference': np.float64(-0.11329316954757428),\n",
       " 'disparate_impact_ratio': 0.4080612722495058,\n",
       " 'base_rate_preds': np.float64(0.14713888821048746),\n",
       " 'equal_opportunity_difference': -0.15186627250442014,\n",
       " 'average_odds_difference': -0.09608825241172816,\n",
       " 'conditional_demographic_disparity': np.float64(-0.04701218213820861),\n",
       " 'smoothed_edf': np.float64(0.8963376749045708),\n",
       " 'df_bias_amplification': np.float64(0.18520092387685327),\n",
       " 'balanced_accuracy_score': 0.7053203897748427}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metrics(\n",
    "    y_true=val_dir.labels[:,0],\n",
    "    y_pred=preds_dir,\n",
    "    prot_attr=val_dir.protected_attributes[:, sens_ind],\n",
    "    pos_label=1,\n",
    "    sample_weight=val_dir.instance_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Question: Apprentissage de représentation latente fair\n",
    "\n",
    "Apprendre le pre-processing et evaluer son impact avec les métriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 1.69019295342257, L_x: 73.81000902947056,  L_y: 0.7422937630397612,  L_z: 0.004195982001762061\n",
      "step: 250, loss: 1.6901930202302262, L_x: 73.810009051626,  L_y: 0.7422937683058747,  L_z: 0.004195983228161832\n",
      "step: 500, loss: 1.6901928993149677, L_x: 73.81000903010553,  L_y: 0.7422937709301678,  L_z: 0.004195980761674894\n",
      "step: 750, loss: 1.4936821803790545, L_x: 73.7777970675713,  L_y: 0.4735322920457071,  L_z: 0.005647438353152684\n",
      "step: 1000, loss: 1.4936821601096708, L_x: 73.77779711332491,  L_y: 0.47353229387508383,  L_z: 0.005647437902026759\n",
      "step: 1250, loss: 1.493682187894152, L_x: 73.77779708111517,  L_y: 0.47353229002863934,  L_z: 0.005647438541087219\n",
      "step: 1500, loss: 1.3349354346450175, L_x: 73.80464716315878,  L_y: 0.46430653993217974,  L_z: 0.0026516484616250003\n",
      "step: 1750, loss: 1.3349352071609666, L_x: 73.8046471783519,  L_y: 0.464306548171032,  L_z: 0.0026516437441283123\n",
      "step: 2000, loss: 1.3349355032783756, L_x: 73.80464714460902,  L_y: 0.4643065428863328,  L_z: 0.002651649778919052\n",
      "step: 2250, loss: 1.6675221710244743, L_x: 73.67024771773472,  L_y: 0.513074516938595,  L_z: 0.00835490353817064\n",
      "step: 2500, loss: 1.6675220026505617, L_x: 73.67024768641053,  L_y: 0.51307451580647,  L_z: 0.008354900199599728\n",
      "step: 2750, loss: 1.6675223540241906, L_x: 73.67024764533622,  L_y: 0.5130745164399497,  L_z: 0.008354907222617577\n",
      "step: 3000, loss: 1.248086161732937, L_x: 73.80324543504634,  L_y: 0.47123613923799246,  L_z: 0.0007763513628896212\n",
      "step: 3250, loss: 1.2480864848437516, L_x: 73.80324543382967,  L_y: 0.47123614623371507,  L_z: 0.000776357685434792\n",
      "step: 3500, loss: 1.3580842175823684, L_x: 73.79420532654302,  L_y: 0.46637809018563264,  L_z: 0.003075281482626113\n",
      "step: 3750, loss: 1.3580843368935005, L_x: 73.79420531895641,  L_y: 0.466378097810519,  L_z: 0.003075283717868349\n",
      "step: 4000, loss: 1.3580841888704676, L_x: 73.79420532879568,  L_y: 0.46637810301986,  L_z: 0.003075280651253015\n",
      "step: 4250, loss: 1.233189278588048, L_x: 73.80416097076926,  L_y: 0.4681904389509084,  L_z: 0.0005391445985889365\n",
      "step: 4500, loss: 1.2331894506956442, L_x: 73.80416098987982,  L_y: 0.46819044637530327,  L_z: 0.0005391478884308575\n",
      "step: 4750, loss: 1.2331894692398209, L_x: 73.80416099654857,  L_y: 0.4681904449672022,  L_z: 0.00053914828614266\n",
      "step: 5000, loss: 1.2248295941053775, L_x: 73.80925628733462,  L_y: 0.46345018339961425,  L_z: 0.0004657369566483438\n",
      "step: 5250, loss: 1.2248293370567491, L_x: 73.80925633926262,  L_y: 0.46345019627100376,  L_z: 0.0004657315478623858\n",
      "step: 5500, loss: 1.2248297946308235, L_x: 73.80925631974753,  L_y: 0.4634501855360673,  L_z: 0.00046574091794561733\n"
     ]
    }
   ],
   "source": [
    "from aif360.algorithms.preprocessing.lfr import LFR\n",
    "TR = LFR(\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups,\n",
    "    k=5,\n",
    "    Ax=0.01,\n",
    "    Ay=1.0,\n",
    "    Az=50.0,\n",
    "    print_interval=250,\n",
    "    verbose=1,\n",
    "    seed=None,\n",
    ")\n",
    "TR = TR.fit(dataset_orig_panel19_train, maxiter=5000, maxfun=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform training data and align features\n",
    "dataset_transf_train = TR.transform(dataset_orig_panel19_train)\n",
    "dataset_transf_val = TR.transform(dataset_orig_panel19_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8078534797262537"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lfr = make_pipeline(StandardScaler(), LogisticRegression(solver='liblinear', random_state=42))\n",
    "\n",
    "model_lfr = model_lfr.fit(\n",
    "    dataset_transf_train.features,\n",
    "    y_train,\n",
    "    **{\"logisticregression__sample_weight\": dataset_transf_train.instance_weights}\n",
    ")\n",
    "\n",
    "preds_lfr = model_lfr.predict(dataset_transf_val.features)\n",
    "\n",
    "model_lfr.score(dataset_transf_val.features, y_val, sample_weight=dataset_transf_val.instance_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8078534797262537, 0.5726977338647012)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_val, preds_lfr, sample_weight=dataset_orig_panel19_val.instance_weights), balanced_accuracy_score(y_val, preds_lfr, sample_weight=dataset_orig_panel19_val.instance_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_rate_truth': np.float64(0.2138698873472541),\n",
       " 'statistical_parity_difference': np.float64(-0.018138050641821286),\n",
       " 'disparate_impact_ratio': 0.667548925004786,\n",
       " 'base_rate_preds': np.float64(0.04747348034760007),\n",
       " 'equal_opportunity_difference': -0.00408179662840899,\n",
       " 'average_odds_difference': -0.0006331560634439265,\n",
       " 'conditional_demographic_disparity': np.float64(-0.020886945234518745),\n",
       " 'smoothed_edf': np.float64(0.40414214278625726),\n",
       " 'df_bias_amplification': np.float64(-0.3069946082414603),\n",
       " 'balanced_accuracy_score': 0.5726977338647012}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metrics(\n",
    "            y_true=y_val,\n",
    "            y_pred=preds_lfr,\n",
    "            prot_attr=dataset_transf_val.protected_attributes[:, sens_ind],\n",
    "            pos_label=1,\n",
    "            sample_weight=dataset_transf_val.instance_weights,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Post processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Question: Use the post-processing Reject Option Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.postprocessing.reject_option_classification import (\n",
    "    RejectOptionClassification,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Reuse the first Logistic Regression learn to find the best threshold that maximises its balanced accuracy on the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best classification threshold of the validation dataset\n",
    "df_val_pred = dataset_orig_panel19_val.copy(deepcopy=True)\n",
    "df_val_pred.scores = model.predict_proba(dataset_orig_panel19_val.features)[:,1].reshape(-1,1)\n",
    "num_thresh = 100\n",
    "ba_arr = np.zeros(num_thresh)\n",
    "class_thresh_arr = np.linspace(0.01, 0.99, num_thresh)\n",
    "for idx, class_thresh in enumerate(class_thresh_arr):\n",
    "    \n",
    "    fav_inds = df_val_pred.scores > class_thresh\n",
    "    df_val_pred.labels[fav_inds] = df_val_pred.favorable_label\n",
    "    df_val_pred.labels[~fav_inds] = df_val_pred.unfavorable_label\n",
    "    \n",
    "    classified_metric_orig_valid = ClassificationMetric(dataset_orig_panel19_val,\n",
    "                                             df_val_pred, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "    ba_arr[idx] = 0.5*(classified_metric_orig_valid.true_positive_rate()\\\n",
    "                       +classified_metric_orig_valid.true_negative_rate())\n",
    "\n",
    "best_ind = np.where(ba_arr == np.max(ba_arr))[0][0]\n",
    "best_class_thresh = class_thresh_arr[best_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' best indice 26, corresponding balanced accuracy 0.778138731707699, and threshold 0.2673737373737374'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\" best indice {best_ind}, corresponding balanced accuracy {ba_arr[best_ind]}, and threshold {best_class_thresh}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Use the RejectOptionClassification  on the validation dataset with the logistic regression predictions. To improve the fairness metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"Statistical parity difference\"\n",
    "metric_ub = 0.05\n",
    "metric_lb = -0.05\n",
    "\n",
    "ROC = RejectOptionClassification(\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups,\n",
    "    low_class_thresh=0.01,\n",
    "    high_class_thresh=0.99,\n",
    "    num_class_thresh=100,\n",
    "    num_ROC_margin=50,\n",
    "    metric_name=metric_name,\n",
    "    metric_ub=metric_ub,\n",
    "    metric_lb=metric_lb,\n",
    ")\n",
    "\n",
    "ROC = ROC.fit(dataset_orig_panel19_val, df_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal classification threshold (with fairness constraints) = 0.1981\n",
      "Optimal ROC margin = 0.0849\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal classification threshold (with fairness constraints) = %.4f\" % ROC.classification_threshold)\n",
    "print(\"Optimal ROC margin = %.4f\" % ROC.ROC_margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_rate_truth': np.float64(0.2138698873472541),\n",
       " 'statistical_parity_difference': np.float64(-0.03941403450831321),\n",
       " 'disparate_impact_ratio': 0.8860683791942646,\n",
       " 'base_rate_preds': np.float64(0.33054875903384817),\n",
       " 'equal_opportunity_difference': 0.040959657046303555,\n",
       " 'average_odds_difference': 0.037069489594593885,\n",
       " 'conditional_demographic_disparity': np.float64(-0.009274892312833162),\n",
       " 'smoothed_edf': np.float64(0.12096113305336464),\n",
       " 'df_bias_amplification': np.float64(-0.5901756179743529),\n",
       " 'balanced_accuracy_score': 0.7686831187962461}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_roc_val_pred = ROC.predict(df_val_pred)\n",
    "get_metrics(\n",
    "    y_true=y_val,\n",
    "    y_pred=df_roc_val_pred.labels[:,0],\n",
    "    prot_attr=df_val_pred.protected_attributes[:, sens_ind],\n",
    "    pos_label=1,\n",
    "    sample_weight=df_val_pred.instance_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7776645020353395, 0.7686831187962461)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_val, df_roc_val_pred.labels[:,0], sample_weight=dataset_orig_panel19_val.instance_weights), balanced_accuracy_score(y_val, df_roc_val_pred.labels[:,0], sample_weight=dataset_orig_panel19_val.instance_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Do the same while starting from the Logistic Regression learned on the Reweighted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best classification threshold of the validation dataset\n",
    "df_val_pred_rw = dataset_orig_panel19_val.copy(deepcopy=True)\n",
    "df_val_pred_rw.scores = model_rw.predict_proba(dataset_orig_panel19_val.features)[:,1].reshape(-1,1)\n",
    "num_thresh = 100\n",
    "ba_arr = np.zeros(num_thresh)\n",
    "class_thresh_arr = np.linspace(0.01, 0.99, num_thresh)\n",
    "for idx, class_thresh in enumerate(class_thresh_arr):\n",
    "    \n",
    "    fav_inds = df_val_pred_rw.scores > class_thresh\n",
    "    df_val_pred_rw.labels[fav_inds] = df_val_pred_rw.favorable_label\n",
    "    df_val_pred_rw.labels[~fav_inds] = df_val_pred_rw.unfavorable_label\n",
    "    \n",
    "    classified_metric_orig_valid = ClassificationMetric(dataset_orig_panel19_val,\n",
    "                                             df_val_pred_rw, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "    ba_arr[idx] = 0.5*(classified_metric_orig_valid.true_positive_rate()\\\n",
    "                       +classified_metric_orig_valid.true_negative_rate())\n",
    "\n",
    "best_ind = np.where(ba_arr == np.max(ba_arr))[0][0]\n",
    "best_class_thresh = class_thresh_arr[best_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' best indice 23, corresponding balanced accuracy 0.772509430826847, and threshold 0.23767676767676768'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\" best indice {best_ind}, corresponding balanced accuracy {ba_arr[best_ind]}, and threshold {best_class_thresh}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC_rw = ROC.fit(dataset_orig_panel19_val, df_val_pred_rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal classification threshold (with fairness constraints) = 0.1981\n",
      "Optimal ROC margin = 0.0162\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal classification threshold (with fairness constraints) = %.4f\" % ROC_rw.classification_threshold)\n",
    "print(\"Optimal ROC margin = %.4f\" % ROC_rw.ROC_margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_rate_truth': np.float64(0.2138698873472541),\n",
       " 'statistical_parity_difference': np.float64(-0.04521374283341156),\n",
       " 'disparate_impact_ratio': 0.8739890065412739,\n",
       " 'base_rate_preds': np.float64(0.34114655451968756),\n",
       " 'equal_opportunity_difference': 0.03326708596015693,\n",
       " 'average_odds_difference': 0.030453342202646116,\n",
       " 'conditional_demographic_disparity': np.float64(-0.010474977876198086),\n",
       " 'smoothed_edf': np.float64(0.13468746106527973),\n",
       " 'df_bias_amplification': np.float64(-0.5764492899624378),\n",
       " 'balanced_accuracy_score': 0.7688763176438125}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_roc_val_pred_rw = ROC.predict(df_val_pred_rw)\n",
    "get_metrics(\n",
    "    y_true=y_val,\n",
    "    y_pred=df_roc_val_pred_rw.labels[:,0],\n",
    "    prot_attr=df_val_pred_rw.protected_attributes[:, sens_ind],\n",
    "    pos_label=1,\n",
    "    sample_weight=df_val_pred_rw.instance_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7717297349507508, 0.7688763176438125)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_val, df_roc_val_pred_rw.labels[:,0], sample_weight=dataset_orig_panel19_val.instance_weights), balanced_accuracy_score(y_val, df_roc_val_pred_rw.labels[:,0], sample_weight=dataset_orig_panel19_val.instance_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Use the Calibrated Equalised Odds  on the validation dataset with the logistic regression predictions. To improve the fairness metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.postprocessing.calibrated_eq_odds_postprocessing import CalibratedEqOddsPostprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Learn parameters to equalize odds and apply to create a new dataset\n",
    "# cost constraint of fnr will optimize generalized false negative rates, that of\n",
    "# fpr will optimize generalized false positive rates, and weighted will optimize\n",
    "# a weighted combination of both\n",
    "cost_constraint = \"fnr\" # \"fnr\", \"fpr\", \"weighted\"\n",
    "cpp = CalibratedEqOddsPostprocessing(privileged_groups = privileged_groups,\n",
    "                                     unprivileged_groups = unprivileged_groups,\n",
    "                                     cost_constraint=cost_constraint,\n",
    "                                     seed=42)\n",
    "cpp = cpp.fit(dataset_orig_panel19_val, df_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_rate_truth': np.float64(0.2138698873472541),\n",
       " 'statistical_parity_difference': np.float64(-0.02918610408073935),\n",
       " 'disparate_impact_ratio': 0.6942182543380743,\n",
       " 'base_rate_preds': np.float64(0.08404684235731487),\n",
       " 'equal_opportunity_difference': 0.06522029850494027,\n",
       " 'average_odds_difference': 0.02819188110283477,\n",
       " 'conditional_demographic_disparity': np.float64(-0.01974212632292331),\n",
       " 'smoothed_edf': np.float64(0.3649686491174231),\n",
       " 'df_bias_amplification': np.float64(-0.34616810191029446),\n",
       " 'balanced_accuracy_score': 0.6203454334235002}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ceqodds_val_pred = cpp.predict(df_val_pred)\n",
    "get_metrics(\n",
    "    y_true=y_val,\n",
    "    y_pred=df_ceqodds_val_pred.labels[:,0],\n",
    "    prot_attr=df_val_pred.protected_attributes[:, sens_ind],\n",
    "    pos_label=1,\n",
    "    sample_weight=df_val_pred.instance_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.818967946129252, 0.6203454334235002)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_val, df_ceqodds_val_pred.labels[:,0], sample_weight=dataset_orig_panel19_val.instance_weights), balanced_accuracy_score(y_val, df_ceqodds_val_pred.labels[:,0], sample_weight=dataset_orig_panel19_val.instance_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpp_rw = cpp.fit(dataset_orig_panel19_val, df_val_pred_rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_rate_truth': np.float64(0.2138698873472541),\n",
       " 'statistical_parity_difference': np.float64(-0.06549596392187172),\n",
       " 'disparate_impact_ratio': 0.6051655183493769,\n",
       " 'base_rate_preds': np.float64(0.14029807811094752),\n",
       " 'equal_opportunity_difference': 0.007035005711165887,\n",
       " 'average_odds_difference': -0.0059817655781470025,\n",
       " 'conditional_demographic_disparity': np.float64(-0.028276612743995587),\n",
       " 'smoothed_edf': np.float64(0.5022531118574891),\n",
       " 'df_bias_amplification': np.float64(-0.20888363917022845),\n",
       " 'balanced_accuracy_score': 0.6886436054359004}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ceqodds_val_pred_rw = cpp_rw.predict(df_val_pred_rw)\n",
    "get_metrics(\n",
    "    y_true=y_val,\n",
    "    y_pred=df_ceqodds_val_pred_rw.labels[:,0],\n",
    "    prot_attr=df_val_pred_rw.protected_attributes[:, sens_ind],\n",
    "    pos_label=1,\n",
    "    sample_weight=df_val_pred_rw.instance_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8327093673467971, 0.6886436054359004)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_val, df_ceqodds_val_pred_rw.labels[:,0], sample_weight=dataset_orig_panel19_val.instance_weights), balanced_accuracy_score(y_val, df_ceqodds_val_pred_rw.labels[:,0], sample_weight=dataset_orig_panel19_val.instance_weights)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1reWieE41k1RU9_T08Chmufh9WOwBoIKZ",
     "timestamp": 1706013971768
    },
    {
     "file_id": "1fWzH-WkCZ9xcagC-8OY71_b_aNcXfdpM",
     "timestamp": 1705973751107
    }
   ]
  },
  "kernelspec": {
   "display_name": "TD_corr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
